{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from typing import List\n",
    "from collections import Counter, OrderedDict\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu as device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} as device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "model_dir = \"./models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True # set to false for full training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    CONTEXT_WINDOW = 2 # the number of words on either side of target word\n",
    "    EMBEDDING_SIZE = 5\n",
    "    MIN_FREQ = 25 # dropping words that appear less than 5 times\n",
    "    BATCH_SIZE = 3\n",
    "    N_EPOCHS = 1\n",
    "else:\n",
    "    CONTEXT_WINDOW = 4 # the number of words on either side of target word\n",
    "    EMBEDDING_SIZE = 100\n",
    "    MIN_FREQ = 25 # dropping words that appear less than 5 times\n",
    "    BATCH_SIZE = 64\n",
    "    N_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://adsp-nlp-open/data/word2vec_training_sentences.json...\n",
      "==> NOTE: You are downloading one or more large file(s), which would            \n",
      "run significantly faster if you enabled sliced object downloads. This\n",
      "feature is enabled by default but requires that compiled crcmod be\n",
      "installed (see \"gsutil help crcmod\").\n",
      "\n",
      "/ [1 files][274.8 MiB/274.8 MiB]    1.2 MiB/s                                   \n",
      "Operation completed over 1 objects/274.8 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "#Extract Data\n",
    "import requests\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/7370/pg7370.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "with open(\"data.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 824,099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['irish', 'league', 'cup'],\n",
       " ['sabre', 'engine', 'thrust', 'weight', 'ratio', 'up', 'to', 'atmospheric'],\n",
       " ['the',\n",
       "  'recording',\n",
       "  'was',\n",
       "  'engineered',\n",
       "  'by',\n",
       "  'jim',\n",
       "  'caruana',\n",
       "  'and',\n",
       "  'mixed',\n",
       "  'by',\n",
       "  'jason',\n",
       "  'goldstein',\n",
       "  'at',\n",
       "  'sony',\n",
       "  'music',\n",
       "  'studios',\n",
       "  'in',\n",
       "  'new',\n",
       "  'york',\n",
       "  'city'],\n",
       " ['goldstein', 'was', 'hired', 'to', 'mix', 'b', 'day'],\n",
       " ['he', 'said', 'this', 'song', 'was', 'really', 'simple', 'to', 'mix']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"data.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "\n",
    "start = text.find(\"The Project Gutenberg eBook of Second Treatise of Government\")\n",
    "# Extract the text starting from the specified line\n",
    "# No footer to remove in this case\n",
    "\n",
    "sentences = text[start:]\n",
    "\n",
    "# Split text into sentences based on periods and remove leading/trailing whitespace\n",
    "sentences = [sentence.strip() for sentence in text.split('.') if sentence.strip()]\n",
    "\n",
    "num_sentences = len(sentences)\n",
    "print(\"Number of sentences:\", num_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentences into words\n",
    "# Sentence and word tokenize, clean, and lower case\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "sentences = sent_tokenize(text)\n",
    "processed_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = word_tokenize(sentence)\n",
    "    words = [re.sub(r'\\W+', '', word.lower()) for word in words if word.isalpha()]\n",
    "    if words:\n",
    "        processed_sentences.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(\n",
    "        self,\n",
    "        word_counts: OrderedDict, # vocabular is based on word counts\n",
    "        min_freq: int = 1, # min times a word must appear in corpus (rare words might not be worth considering)\n",
    "        max_size: int = None, # we can limit the amount of words as well \n",
    "        specials: List[str] = None, # any other special tokens we may want to add, like padding tokens\n",
    "        unk_token: str = \"<unk>\" # reserved token for when we run into words not in the vocabulary\n",
    "    ):\n",
    "        self.word_counts = word_counts\n",
    "        self.min_freq = min_freq\n",
    "        self.max_size = max_size\n",
    "        self.unk_token = unk_token\n",
    "        self.specials = list(specials) if specials else []\n",
    "\n",
    "        if self.unk_token not in self.specials:\n",
    "            self.specials.insert(0, self.unk_token) # unknown token should always be included\n",
    "\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = []\n",
    "\n",
    "        self._prepare_vocab()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2token)\n",
    "    \n",
    "\n",
    "    def __contains__(self, value):\n",
    "        return value in self.idx2token\n",
    "\n",
    "\n",
    "    def _prepare_vocab(self):\n",
    "        \"\"\"Processes input OrderedDict: Filters based on min_freq & adds special tokens.\"\"\"\n",
    "        vocab_list = self.specials.copy()  # Copy specials to avoid modifying original list\n",
    "\n",
    "        # filter words based on min_freq and add to vocab\n",
    "        filtered_words = [\n",
    "            word\n",
    "            for word, freq in self.word_counts.items()\n",
    "            if freq >= self.min_freq and word not in self.specials\n",
    "        ]\n",
    "\n",
    "        # enforcing max vocab size constraint\n",
    "        if self.max_size is not None:\n",
    "            n_to_keep = self.max_size - len(self.specials) # special tokens take up spaces\n",
    "            filtered_words = filtered_words[:n_to_keep]\n",
    "\n",
    "        # creating final vocab list\n",
    "        vocab_list.extend(word for word in filtered_words)\n",
    "\n",
    "        # create look up tables\n",
    "        self.idx2token = vocab_list\n",
    "        self.token2idx = {word: idx for idx, word in enumerate(vocab_list)}\n",
    "\n",
    "\n",
    "    def get_token(self, idx: int) -> str:\n",
    "        \"\"\"Returns the token corresponding to an index. Raises error if index is out of range.\"\"\"\n",
    "        if 0 <= idx < len(self.idx2token):\n",
    "            return self.idx2token[idx]\n",
    "        raise IndexError(f\"Index {idx} is out of range for vocabulary size {len(self.idx2token)}\")\n",
    "\n",
    "\n",
    "    def get_index(self, token: str) -> int:\n",
    "        \"\"\"Returns the index corresponding to a token. Defaults to unk_token if missing.\"\"\"\n",
    "        return self.token2idx.get(token, self.token2idx[self.unk_token])  # return unk_token index if word is not in vocab\n",
    "\n",
    "\n",
    "    def get_tokens(self, indices: List[int]) -> List[str]:\n",
    "        \"\"\"Converts a list of indices into a list of tokens.\"\"\"\n",
    "        return [self.get_token(idx) for idx in indices]\n",
    "\n",
    "\n",
    "    def get_indices(self, tokens: List[str]) -> List[int]:\n",
    "        \"\"\"Converts a list of tokens into a list of indices.\"\"\"\n",
    "        return [self.get_index(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_sentences(sentences: List[List[str]], context_length: int, pad_token: str = \"<pad>\") -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Pads each sentence to fit the context window length with the literal string \"<pad>\".\n",
    "    \n",
    "    Args:\n",
    "        sentences: A list of sentences, where each sentence is a list of tokens.\n",
    "        context_length: The number of tokens to either side of the target token.\n",
    "\n",
    "    Returns:\n",
    "        A list of padded sentences.\n",
    "    \"\"\"\n",
    "    padded_sentences = []\n",
    "    for sentence in sentences:\n",
    "        padded_sentence = [pad_token] * context_length + sentence + [pad_token] * context_length\n",
    "        padded_sentences.append(padded_sentence)\n",
    "    \n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = pad_sentences(processed_sentences, CONTEXT_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<pad>', '<pad>', 'irish', 'league', 'cup', '<pad>', '<pad>'],\n",
       " ['<pad>',\n",
       "  '<pad>',\n",
       "  'sabre',\n",
       "  'engine',\n",
       "  'thrust',\n",
       "  'weight',\n",
       "  'ratio',\n",
       "  'up',\n",
       "  'to',\n",
       "  'atmospheric',\n",
       "  '<pad>',\n",
       "  '<pad>']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(\n",
    "    word_counts=OrderedDict(Counter(chain.from_iterable(sentences))),\n",
    "    min_freq=MIN_FREQ,\n",
    "    specials=[\"<pad>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 28,322\n"
     ]
    }
   ],
   "source": [
    "# creating a vocabulary\n",
    "print(f\"Size of Vocabulary: {len(vocab):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0 corresponds to `<unk>`\n",
      "Index 1 corresponds to `<pad>`\n",
      "Index 5 corresponds to `sabre`\n",
      "Index 500 corresponds to `insects`\n",
      "Index 10000 corresponds to `dwarf`\n"
     ]
    }
   ],
   "source": [
    "for idx in [0, 1, 5, 500, 10_000]:\n",
    "    print(f\"Index {idx} corresponds to `{vocab.get_token(idx)}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we prepare the training data. For skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Tuple\n",
    "\n",
    "def generate_skipgram_pairs(sentences: List[List[str]], context_length: int, vocab) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate (center, context) pairs for Skip-gram model.\n",
    "\n",
    "    Args:\n",
    "        sentences: A list of sentences, where each sentence is a list of tokens.\n",
    "        context_length: The number of tokens to either side of the center token.\n",
    "        vocab: A vocab object that maps words to indices and vice versa.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of two torch.Tensors:\n",
    "            - centers: tensor of center word indices\n",
    "            - contexts: tensor of corresponding context word indices\n",
    "    \"\"\"\n",
    "    \n",
    "    centers = []\n",
    "    contexts = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        enc_sentence = vocab.get_indices(sentence)\n",
    "\n",
    "        for center_idx in range(context_length, len(enc_sentence) - context_length):\n",
    "            center = enc_sentence[center_idx]\n",
    "\n",
    "            # Iterate over each context word around the center\n",
    "            for i in range(-context_length, context_length + 1):\n",
    "                if i == 0:\n",
    "                    continue  # skip the center word itself\n",
    "                context = enc_sentence[center_idx + i]\n",
    "                centers.append(center)\n",
    "                contexts.append(context)\n",
    "\n",
    "    return torch.tensor(centers), torch.tensor(contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: torch.Size([16409899])\n"
     ]
    }
   ],
   "source": [
    "centerss, contexts = generate_skipgram_pairs(sentences, CONTEXT_WINDOW, vocab)\n",
    "\n",
    "print(f\"Number of center-context pairs: {len(centerss):,}\")\n",
    "print(f\"First 5 center indices: {centerss[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contexts:\n",
      "tensor([[   1,    1,    3,    4],\n",
      "        [   1,    2,    4,    1],\n",
      "        [   2,    3,    1,    1],\n",
      "        ...,\n",
      "        [   1,    0, 1732,    1],\n",
      "        [   0,  768,    1,    1],\n",
      "        [   1,    1,    1,    1]])\n",
      "contexts shape: torch.Size([16409899, 4])\n"
     ]
    }
   ],
   "source": [
    "# does does context look like?\n",
    "print(\"contexts:\", contexts, sep=\"\\n\")\n",
    "print(\"contexts shape:\", contexts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contexts: tensor([   2,    3,    4,  ...,  768, 1732, 3001])\n",
      "contexts shape: torch.Size([16409899])\n"
     ]
    }
   ],
   "source": [
    "# does do targets look like?\n",
    "print(\"contexts:\", targets)\n",
    "print(\"contexts shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: ['engine', 'thrust', 'ratio', 'up']\n",
      "target: ['weight']\n",
      "\n",
      "context: ['music', 'studios', 'new', 'york']\n",
      "target: ['in']\n",
      "\n",
      "context: ['to', 'favour', 'by', 'birds']\n",
      "target: ['visits']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# converting first context-target pair back to string\n",
    "for idx in [6, 27, 1000]:\n",
    "    print(\"context:\", vocab.get_tokens(contexts[idx].tolist()))\n",
    "    print(\"target:\", vocab.get_tokens([targets[idx].item()]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a custome Dataset object so we can wrap it in a DataLoader object for batching, shuffling etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NGramDataset(Dataset): # subclassing Dataset is required here\n",
    "    \n",
    "    def __init__(self, contexts, targets): # necessary method / function\n",
    "        self.contexts = contexts\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self): # necessary method / function\n",
    "        return len(self.contexts)\n",
    "\n",
    "    def __getitem__(self, idx): # necessary method / function\n",
    "        return self.contexts[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the CBOW model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NGramDataset(vocab_size=len(vocab)).to(device)\n",
    "print(model)\n",
    "print(f\"Size of Vocabulary: {len(vocab):,}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-nlp-nlp",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "adsp-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
